<div class="section">
  <p>The shortest possible answer is,<br><a href="https://linuxcommandlibrary.com/basic/oneliners">https://linuxcommandlibrary.com/basic/oneliners</a> to get a good overview of what they are typing.</p>
  <p>It is a different kind of a user interface,<br>and the applications within that world can be connected together to make new applications.</p>
  <p>This used to be the original computer interface,<br>before graphic applications and the mouse were invented.</p>
  <p>Today it is used because it is faster than the GUI,<br>it is less distracting, and it is easier to make those programs.</p>
</div>
<div class="section">
  <hr>
</div>
<div class="section">
  <p>Think about playing music, how many icons you have to click to get there,<br>on the command line you just type in <code>cvlc ~/Music/*</code> and keep doing what you were doing.</p>
  <p>The reason why graphic user interfaces are popular,<br>is because they have an easy on ramp, they give you breadcrumbs.</p>
  <p>The command line requires that you remember where your files are,<br>and remember some of the command names, such as cvlc to play music, or wget to download a file.</p>
</div>
<div class="section">
  <hr>
</div>
<div class="section">
  <p>You don't have to memorize those commands,<br><a href="https://www.google.com/search?q=Linux+Cheat+Sheet">you can just download a little cheat-sheet</a>.</p>
  <p>After a while you will stop forgetting their names,<br>it is really not that big of a deal.</p>
  <p>And if you need to browse some photos,<br>then you just use a normal GUI program that can display them.</p>
  <p>Though, instead of clicking on an icon somewhere,<br>you'll launch it from the command line by entering its name.</p>
  <p>It is just another command to add to your cheat sheet,<br>and just like there is a limited number of things that you do on your phone.</p>
  <p>You'll also limit yourself to just knowing 5 to 20 things,<br>that you normally do on the command line.</p>
</div>
<div class="section">
  <hr>
</div>
<div class="section">
  <p>Just like your phone apps or desktop applications,<br>programs on the command line can do multiple things.</p>
  <p>every command has a help feature,<br>where you type in the command name followed by <code>-h</code> and that will list all the different things that a command can do.</p>
  <p>For example <a href="https://www.guyrutenberg.com/2014/05/02/make-offline-mirror-of-a-site-using-wget/">wget has a --mirror option</a>,<br>here you don't just download a file, but mirror the entire website.</p>
  <p>So just imagine wget being an app on your phone, called Web Get,<br>and it would have two buttons, "Download A File", and "Download Website".</p>
</div>
<div class="section">
  <hr>
</div>
<div class="section">
  <p>Now let me give you a more involved example,<br>I would like to show you how to download internet stories.</p>
  <p>Every website offers some way of getting some of their data,<br>Hacker News (a website for programmers) has a nice description of their raw data: <a href="https://github.com/HackerNews/API">https://github.com/HackerNews/API</a></p>
  <p>We learn that their website data structure is all about numbers,<br>and to get at those number we have to download their <a href="https://hacker-news.firebaseio.com/v0/topstories.json?print=pretty">topstories.json</a></p>
  <p>JSON is a file format, it is not really purrfect for the command line,<br>so we will thin it, reduce it simplify with a program called <a href="https://stedolan.github.io/jq/">jq</a>,</p>
  <p>jq reaches into the JSON file format and grabs plain text for us,<br>without the extra curly bracets, quotes, or in this case, commas that delimit the entries and square brackets that represent lists of data.</p>
  <p>We will use <a href="https://www.youtube.com/watch?v=I6id1Y0YuNk">curl</a>, it is very similar to wget,<br>but it doesn't have that fancy extra stuff for mirroring websites.</p>
  <p>And we need a <a href="https://stedolan.github.io/jq/manual/#Basicfilters">jq tutorial</a> to give us some hints on how to use it, <a href="https://www.youtube.com/watch?v=FSn_38gDvzM">sometimes watching a video tutorial is useful too</a>,<br>I found the stuff under "Array/Object Value Iterator: .[]".</p>
  <p>Programmers speak like this when they assume only other programmers will read their stuff,<br>little do they know, we don't really read their funk, we just look at the code.</p>
</div>
<div class="section">
  <hr>
</div>
<div class="section">
  <p>That is an important lesson, never read the freaking manual,<br>what takes you 20 minutes instead of 1 minute, teaches you 20 times more.</p>
  <p>There is a lot of people out there that say <a href="http://www.catb.org/~esr/jargon/html/R/RTFM.html">RTFM</a>,<br>but that is only because they don't know the answer, because they spent their career brainlessly looking everything up - just ignore them.</p>
</div>
<div class="section">
  <hr>
</div>
<div class="section">
  <p>So to get at the numbers we have to say<br>curl <a href="https://hacker-news.firebaseio.com/v0/topstories.json?print=pretty">https://hacker-news.firebaseio.com/v0/topstories.json?print=pretty</a> | jq '.[]'.</p>
  <p>Those single quotes tell your command line that your code in there belong to jq,<br>the command line is a programming language it self, to keep command related stuff isolated we use single brackets.</p>
</div>
<div class="section">
  <hr>
</div>
<div class="section">
  <p>The way you get a feel for this is by looking at <a href="https://www.google.com/search?q=command+line+cheat+sheets">command line cheat sheets</a>,<br>and <a href="https://www.youtube.com/results?search_query=Linux+command+line+tutorial">video tutorials</a>, and if you are going somewhere without computers, <a href="https://www.google.com/search?q=linux+pocket+reference">grab some pocket reference titles</a>.</p>
  <p>The smaller the source of information,<br>the more valuable the information within.</p>
  <p>If you buy a Linux Bible,<br>you won't be able to tell what is important and what isn't.</p>
  <p>And it is really cool to just have a long little row,<br>of pocket reference books, it shows that you don't mess around.</p>
</div>
<div class="section">
  <hr>
</div>
<div class="section">
  <p>At this point we have a lit of numbers that represent stories on hacker news,<br>to grab a story we have to curl item / story number followed by .json <a href="https://hacker-news.firebaseio.com/v0/item/27941616.json?print=pretty">27941616.json</a></p>
  <p>There is a bit of a problem here, because piping the output of the list of numbers to,<br>wc command with the -l switch, which stands for word count, count number of lines for me.</p>
  <p>Reveals that hacker news API returns 500 story numbers,<br>and it is bad etiquette to issue 500 requests to hacker news servers.</p>
  <p>Moreover, each hacker news story has, a list of numbers that represent the comments,<br>presumably the top comments in the thread.</p>
  <p>Even though they say there is no rate limit,<br>we would probably trigger their security system and get temporary banned.</p>
</div>
<div class="section">
  <hr>
</div>
<div class="section">
  <p>I recently got banned by npm becasue i was looking for package names I could use to publish a program,<br>I knew I wanted something similar to rsync, which is the name of a command responsible for remote synchronization.</p>
  <p>But somebody else took rsync ages ago, so I started changing the url in the address bar to<br>ssync tsync usync vsync wsync xsync ysync,</p>
  <p>And all of a sudden the website said, you are doing too much of that,<br>you are temporary banned - fair enough, I guess.</p>
  <p>I ended up publishing <a href="https://github.com/catpea/rsend">rsend</a>, because it occurred to me,<br>that I am not really syncing arbitrary locations, but sending from local to remote.</p>
  <p>I use it, to upload my music to my workout audio player,<br>but the program is very ugly because, I only expect GNU utilities on the target system.</p>
  <p>While I can program all kinds of things on the local system,<br>the remote system will not have my code, so checking file uploads has to be done via Linux commands.</p>
</div>
<div class="section">
  <hr>
</div>
<div class="section">
  <p>One way to throttle our requests, is to use the sleep command, between downloading each of the stories,<br>we could sleep for 1 second for the first 10 stories, and then, two seconds, three seconds, and finally five seconds between each request, and nobody would get mad a that.</p>
  <p>Another way to do it, is to pre-fetch three or five stories,<br>ahead of where you are at right now, so you wouldn't have any wait times, if you just read the stories live.</p>
  <p>If all you want to see for the day is 10 stories,<br>then you will only download 15, AND NOT 500!</p>
  <p>Beyond that, you could do a little bit of maneuvering,<br>and begin gathering top stories every hour throughout the day at a comfy 10 second interval.</p>
</div>
<div class="section">
  <hr>
</div>
<div class="section">
  <p>There is something you should know,<br>you know how that wc command that we just rand had a -l to make it couldn't lines.</p>
  <p>When you are inside a full screen program,<br>interacting with things kind of should be maybe about letters as well.</p>
  <p>If we made an interactive terminal program,<br>that just browsers a screenful at a time.</p>
  <p>To go to the next page,<br>you would have to pres the "n" key.</p>
  <p>This is yet another Black Screen shortcut,<br>that goes straight into a cheeatsheet.</p>
  <p>We also need to consider other news sources,<br>whatever your favorite hangout spots are.</p>
  <p>And, not everyone is going to be as nice as Hacker News and present an API,<br>there may not be a public API.</p>
  <p>In that case we will need to write a screen scraper,<br>that is one of the worst kinds of programs.</p>
  <p>As it doesn't use the data beneath the screen,<br>it just sort of does this ugly scrape of whatever is on the screen.</p>
  <p>This entails controlling a full Web Browser,<br>and sort of making it look like there is a human browsing the page, where it is actually a freaking robot.</p>
  <p>And it gets worse,<br>lyke super bad.</p>
  <p>There was a world wide DNS problem couple of days ago,<br>and Amazon keeps forcing me to solve their CAPTCHA when I use Firefox, Chromium works fine for me.</p>
  <p>There maybe websites out there that can stop you from scraping them,<br>by presenting a CAPTCHA, and the only way to fix that, is to solve it.</p>
  <p>The glitched Amazon that I am still experiencing,<br>would only ask me to solve the CAPTCHA once per opening a browser, and that was already pretty annoying.</p>
  <p>CAPTCHA is unlikely to feature on News Websites though,<br>but if you scrape too quickly, of they detect your browser signature you may bet one.</p>
  <p>Now that all websites use JavaScript, you can't just mirror them with wget,<br>they need an automated browser that will run the JavaScript to make the pages come to life.</p>
</div>
<div class="section">
  <hr>
</div>
<div class="section">
  <p>When we are inventing on the command line,<br>we have to jump ahead of ourselves and see what our program will need when it is finished.</p>
  <p>And the first thing that comes to mind,<br>is multiple news sources.</p>
  <p>If we are to create a system to extract interesting stories from multiple places on the internet,<br>then it is definable something that should consist of multiple programs, that run hourly or so.</p>
  <p>So we would have one program for extracting data from hacker news,<br>another for extracting data from eBay about some obscure phone that is cool but never available.</p>
  <p>And another program to get emails from different service providers,<br>news from, say, the Raspberry PI blog.</p>
  <p>And information from Raspberry Pi, like temperature or pressure changes,<br>maybe a new hires photos from the nearby Squirrel cam.</p>
</div>
<div class="section">
  <hr>
</div>
<div class="section">
  <p>We could make this work via a command line RSS reader (with the option of using a proper desktop one),<br>by setting up a local RSS server that keeps track of data from all these places.</p>
  <p>And I think the data would have to sit in an SQL database,<br>this way all those different data sources would have a universal format that can be easily processed.</p>
  <p>Once they are put into the database,<br>creating custom RSS feeds is easy, they are actually just plain XML files, same idea as JSON.</p>
</div>
<div class="section">
  <hr>
</div>
<div class="section">
  <p>While SQL and the feed server can be easily can be easily controlled from the command line,<br>the scraping of the various unfriendly websites, will require a custom program.</p>
  <p>Personally I would base it on <a href="https://browserless.js.org/">Browserless.js</a>,<br>as it has built-in "evasions technniques (sic)" to prevent being blocked.</p>
  <p>I just noticed that they misspelled evasion techniques,<br>forked their project, and fixed a couple more mistakes I could find.</p>
  <p><a href="https://github.com/microlinkhq/browserless/pull/295">I created a PR, short for pull request, which means that I am requesting that they pull the fixes that I have created</a>,<br>so because Browserless.js is an Open Source Code Project, anybody can make valuable little contributions.</p>
  <p>But even with the mighty Browserless.js project,<br>websites we would be scraping will sometimes update their code, and our scraper brains would need to be updated.</p>
</div>
<div class="section">
  <hr>
</div>
<div class="section">
  <p>Overall, our best option here, is to cut 10 entries from top of the Hacker News,<br>and see if we can get away with making 10 quick requests for their top stories, and call our initial version done.</p>
  <p>Personally I would begin testing <a href="https://ostechnix.com/newsbeuter-command-line-rssatom-feed-reader-unix-like-systems/">Newsbeuter</a> <a href="https://github.com/msharov/snownews">Snownews</a> <a href="https://www.tecmint.com/newsroom-commandline-linux-news-reader/">Newsroom</a> and <a href="https://www.tecmint.com/newsboat-rss-atom-feed-reader-for-linux-terminals/">newsboat</a> to get the feel for command line news,<br><a href="https://www.youtube.com/watch?v=FSn_38gDvzM">and consider moving to desktop RSS clients</a> so as long as they support ad blockers.</p>
  <p>And then quietly, I would begin working on my scrapers,<br>being mindful that they must output RSS feeds, which then I would add to my RSS readers.</p>
</div>
<div class="section">
  <hr>
</div>
<div class="section">
  <p>To be honest, there is not much time that I have for reading internet news,<br>and the more I think about it, the more fetching that little 10 news items program seems, and the more clunky RSS starts to look.</p>
  <p>Let us chop off the top then entries from Hacker News,<br>and see if we can quickly get an overview of what is going on in the world.</p>
  <p>The command for getting the top of data is head, and to specify how many lines we want we use -n,<br>so our command now looks like this: curl <a href="https://hacker-news.firebaseio.com/v0/topstories.json?print=pretty">https://hacker-news.firebaseio.com/v0/topstories.json?print=pretty</a> | jq '.[]' | head -n 10;</p>
  <p>Armed with 10 numbers that represent top stories,<br>we now have to take the second step, and use each of those numbers to download the titles from Hacker News.</p>
  <p>And here we reuse curl, but the url that we give to curl needs to have that number inserted in there,<br>there are two ways to do this, one is xargs, and the other a while loop.</p>
  <p>The while loop is better, it is more readable, it is faster, but I don't like it because it wraps things,<br>my xargs approach, creates a single straight command line.</p>
  <p>Unfortunately my line ends up having a command that uses quotes, inside quotes,<br>so there is quote escaping, by means of backslash, and that is something I don't like.</p>
  <p>But I must say the proper way to do this, is a single pipe,<br>get the top story numbers, map the numbers to data, and map the data to text so that it looks nice on the black screen.</p>
  <p>Three simple steps,<br>get numbers, convert them to stories, print them to screen.</p>
  <p>In closing, I will read to you the code of each of the three approaches, in the way that I think about it,<br>I wont read quote, backslash, n, backslash n, but give you a somewhat poetic reading.</p>
</div>
<div class="section">
  <hr>
</div>
<div class="section">
  <p>First is my xargs approach, a single pipe;</p>
  <p>curl -s <code>https://hacker-news.firebaseio.com/v0/topstories.json</code> | jq '.[]' | head -n 10 | xargs -I % sh -c "curl -s <code>https://hacker-news.firebaseio.com/v0/item/%.json</code> | jq -r '. | [.title,.url, null] | join("\n")'"</p>
  <p>Now, describe the first loop approach, which is actually a little bit backwards.</p>
  <p>while read -r ID; do curl -s <code>https://hacker-news.firebaseio.com/v0/item/$ID.json</code> | jq -r '. | [.title, "\n", .url, "\n"] | add'; done &lt; &lt;(curl -s <code>https://hacker-news.firebaseio.com/v0/topstories.json</code> | jq '.[]' | head -n 10 )</p>
  <p>And here is the way that people probably prefer, but I am unhappy about, because there shoudn't be a loop here.</p>
  <p>curl -s <code>https://hacker-news.firebaseio.com/v0/topstories.json</code> | jq '.[]' | head -n 10 | while read -r ID ; do curl -s <code>https://hacker-news.firebaseio.com/v0/item/$ID.json</code> | jq -r '. | [.title, "\n", .url, "\n"] | add'; done;</p>
  <p>And that is what the black screens are all about,<br>I hope I inspired you to look a little bit more into those black screens.</p>
</div>