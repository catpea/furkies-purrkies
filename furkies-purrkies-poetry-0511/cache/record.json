{
  "name": "furkies-purrkies-poetry-0511",
  "id": "furkies-purrkies-poetry-0511",
  "title": "Why And How Do Hackers Use Those Black Screens?",
  "date": "2021-07-24T17:54:53.655Z",
  "image": "poetry-0511-illustration.jpg",
  "artwork": "https://unsplash.com/photos/9i5zipnKJ14",
  "audio": "poetry-0511.mp3",
  "guid": "aff797ea-13e2-43e4-b67f-9105754f4f4e",
  "html": "<div class=\"section\">\n  <p>The shortest possible answer is,<br><a href=\"https://linuxcommandlibrary.com/basic/oneliners\">https://linuxcommandlibrary.com/basic/oneliners</a> to get a good overview of what they are typing.</p>\n  <p>It is a different kind of a user interface,<br>and the applications within that world can be connected together to make new applications.</p>\n  <p>This used to be the original computer interface,<br>before graphic applications and the mouse were invented.</p>\n  <p>Today it is used because it is faster than the GUI,<br>it is less distracting, and it is easier to make those programs.</p>\n</div>\n<div class=\"section\">\n  <hr>\n</div>\n<div class=\"section\">\n  <p>Think about playing music, how many icons you have to click to get there,<br>on the command line you just type in <code>cvlc ~/Music/*</code> and keep doing what you were doing.</p>\n  <p>The reason why graphic user interfaces are popular,<br>is because they have an easy on ramp, they give you breadcrumbs.</p>\n  <p>The command line requires that you remember where your files are,<br>and remember some of the command names, such as cvlc to play music, or wget to download a file.</p>\n</div>\n<div class=\"section\">\n  <hr>\n</div>\n<div class=\"section\">\n  <p>You don't have to memorize those commands,<br><a href=\"https://www.google.com/search?q=Linux+Cheat+Sheet\">you can just download a little cheat-sheet</a>.</p>\n  <p>After a while you will stop forgetting their names,<br>it is really not that big of a deal.</p>\n  <p>And if you need to browse some photos,<br>then you just use a normal GUI program that can display them.</p>\n  <p>Though, instead of clicking on an icon somewhere,<br>you'll launch it from the command line by entering its name.</p>\n  <p>It is just another command to add to your cheat sheet,<br>and just like there is a limited number of things that you do on your phone.</p>\n  <p>You'll also limit yourself to just knowing 5 to 20 things,<br>that you normally do on the command line.</p>\n</div>\n<div class=\"section\">\n  <hr>\n</div>\n<div class=\"section\">\n  <p>Just like your phone apps or desktop applications,<br>programs on the command line can do multiple things.</p>\n  <p>every command has a help feature,<br>where you type in the command name followed by <code>-h</code> and that will list all the different things that a command can do.</p>\n  <p>For example <a href=\"https://www.guyrutenberg.com/2014/05/02/make-offline-mirror-of-a-site-using-wget/\">wget has a --mirror option</a>,<br>here you don't just download a file, but mirror the entire website.</p>\n  <p>So just imagine wget being an app on your phone, called Web Get,<br>and it would have two buttons, \"Download A File\", and \"Download Website\".</p>\n</div>\n<div class=\"section\">\n  <hr>\n</div>\n<div class=\"section\">\n  <p>Now let me give you a more involved example,<br>I would like to show you how to download internet stories.</p>\n  <p>Every website offers some way of getting some of their data,<br>Hacker News (a website for programmers) has a nice description of their raw data: <a href=\"https://github.com/HackerNews/API\">https://github.com/HackerNews/API</a></p>\n  <p>We learn that their website data structure is all about numbers,<br>and to get at those number we have to download their <a href=\"https://hacker-news.firebaseio.com/v0/topstories.json?print=pretty\">topstories.json</a></p>\n  <p>JSON is a file format, it is not really purrfect for the command line,<br>so we will thin it, reduce it simplify with a program called <a href=\"https://stedolan.github.io/jq/\">jq</a>,</p>\n  <p>jq reaches into the JSON file format and grabs plain text for us,<br>without the extra curly bracets, quotes, or in this case, commas that delimit the entries and square brackets that represent lists of data.</p>\n  <p>We will use <a href=\"https://www.youtube.com/watch?v=I6id1Y0YuNk\">curl</a>, it is very similar to wget,<br>but it doesn't have that fancy extra stuff for mirroring websites.</p>\n  <p>And we need a <a href=\"https://stedolan.github.io/jq/manual/#Basicfilters\">jq tutorial</a> to give us some hints on how to use it, <a href=\"https://www.youtube.com/watch?v=FSn_38gDvzM\">sometimes watching a video tutorial is useful too</a>,<br>I found the stuff under \"Array/Object Value Iterator: .[]\".</p>\n  <p>Programmers speak like this when they assume only other programmers will read their stuff,<br>little do they know, we don't really read their funk, we just look at the code.</p>\n</div>\n<div class=\"section\">\n  <hr>\n</div>\n<div class=\"section\">\n  <p>That is an important lesson, never read the freaking manual,<br>what takes you 20 minutes instead of 1 minute, teaches you 20 times more.</p>\n  <p>There is a lot of people out there that say <a href=\"http://www.catb.org/~esr/jargon/html/R/RTFM.html\">RTFM</a>,<br>but that is only because they don't know the answer, because they spent their career brainlessly looking everything up - just ignore them.</p>\n</div>\n<div class=\"section\">\n  <hr>\n</div>\n<div class=\"section\">\n  <p>So to get at the numbers we have to say<br>curl <a href=\"https://hacker-news.firebaseio.com/v0/topstories.json?print=pretty\">https://hacker-news.firebaseio.com/v0/topstories.json?print=pretty</a> | jq '.[]'.</p>\n  <p>Those single quotes tell your command line that your code in there belong to jq,<br>the command line is a programming language it self, to keep command related stuff isolated we use single brackets.</p>\n</div>\n<div class=\"section\">\n  <hr>\n</div>\n<div class=\"section\">\n  <p>The way you get a feel for this is by looking at <a href=\"https://www.google.com/search?q=command+line+cheat+sheets\">command line cheat sheets</a>,<br>and <a href=\"https://www.youtube.com/results?search_query=Linux+command+line+tutorial\">video tutorials</a>, and if you are going somewhere without computers, <a href=\"https://www.google.com/search?q=linux+pocket+reference\">grab some pocket reference titles</a>.</p>\n  <p>The smaller the source of information,<br>the more valuable the information within.</p>\n  <p>If you buy a Linux Bible,<br>you won't be able to tell what is important and what isn't.</p>\n  <p>And it is really cool to just have a long little row,<br>of pocket reference books, it shows that you don't mess around.</p>\n</div>\n<div class=\"section\">\n  <hr>\n</div>\n<div class=\"section\">\n  <p>At this point we have a lit of numbers that represent stories on hacker news,<br>to grab a story we have to curl item / story number followed by .json <a href=\"https://hacker-news.firebaseio.com/v0/item/27941616.json?print=pretty\">27941616.json</a></p>\n  <p>There is a bit of a problem here, because piping the output of the list of numbers to,<br>wc command with the -l switch, which stands for word count, count number of lines for me.</p>\n  <p>Reveals that hacker news API returns 500 story numbers,<br>and it is bad etiquette to issue 500 requests to hacker news servers.</p>\n  <p>Moreover, each hacker news story has, a list of numbers that represent the comments,<br>presumably the top comments in the thread.</p>\n  <p>Even though they say there is no rate limit,<br>we would probably trigger their security system and get temporary banned.</p>\n</div>\n<div class=\"section\">\n  <hr>\n</div>\n<div class=\"section\">\n  <p>I recently got banned by npm becasue i was looking for package names I could use to publish a program,<br>I knew I wanted something similar to rsync, which is the name of a command responsible for remote synchronization.</p>\n  <p>But somebody else took rsync ages ago, so I started changing the url in the address bar to<br>ssync tsync usync vsync wsync xsync ysync,</p>\n  <p>And all of a sudden the website said, you are doing too much of that,<br>you are temporary banned - fair enough, I guess.</p>\n  <p>I ended up publishing <a href=\"https://github.com/catpea/rsend\">rsend</a>, because it occurred to me,<br>that I am not really syncing arbitrary locations, but sending from local to remote.</p>\n  <p>I use it, to upload my music to my workout audio player,<br>but the program is very ugly because, I only expect GNU utilities on the target system.</p>\n  <p>While I can program all kinds of things on the local system,<br>the remote system will not have my code, so checking file uploads has to be done via Linux commands.</p>\n</div>\n<div class=\"section\">\n  <hr>\n</div>\n<div class=\"section\">\n  <p>One way to throttle our requests, is to use the sleep command, between downloading each of the stories,<br>we could sleep for 1 second for the first 10 stories, and then, two seconds, three seconds, and finally five seconds between each request, and nobody would get mad a that.</p>\n  <p>Another way to do it, is to pre-fetch three or five stories,<br>ahead of where you are at right now, so you wouldn't have any wait times, if you just read the stories live.</p>\n  <p>If all you want to see for the day is 10 stories,<br>then you will only download 15, AND NOT 500!</p>\n  <p>Beyond that, you could do a little bit of maneuvering,<br>and begin gathering top stories every hour throughout the day at a comfy 10 second interval.</p>\n</div>\n<div class=\"section\">\n  <hr>\n</div>\n<div class=\"section\">\n  <p>There is something you should know,<br>you know how that wc command that we just rand had a -l to make it couldn't lines.</p>\n  <p>When you are inside a full screen program,<br>interacting with things kind of should be maybe about letters as well.</p>\n  <p>If we made an interactive terminal program,<br>that just browsers a screenful at a time.</p>\n  <p>To go to the next page,<br>you would have to pres the \"n\" key.</p>\n  <p>This is yet another Black Screen shortcut,<br>that goes straight into a cheeatsheet.</p>\n  <p>We also need to consider other news sources,<br>whatever your favorite hangout spots are.</p>\n  <p>And, not everyone is going to be as nice as Hacker News and present an API,<br>there may not be a public API.</p>\n  <p>In that case we will need to write a screen scraper,<br>that is one of the worst kinds of programs.</p>\n  <p>As it doesn't use the data beneath the screen,<br>it just sort of does this ugly scrape of whatever is on the screen.</p>\n  <p>This entails controlling a full Web Browser,<br>and sort of making it look like there is a human browsing the page, where it is actually a freaking robot.</p>\n  <p>And it gets worse,<br>lyke super bad.</p>\n  <p>There was a world wide DNS problem couple of days ago,<br>and Amazon keeps forcing me to solve their CAPTCHA when I use Firefox, Chromium works fine for me.</p>\n  <p>There maybe websites out there that can stop you from scraping them,<br>by presenting a CAPTCHA, and the only way to fix that, is to solve it.</p>\n  <p>The glitched Amazon that I am still experiencing,<br>would only ask me to solve the CAPTCHA once per opening a browser, and that was already pretty annoying.</p>\n  <p>CAPTCHA is unlikely to feature on News Websites though,<br>but if you scrape too quickly, of they detect your browser signature you may bet one.</p>\n  <p>Now that all websites use JavaScript, you can't just mirror them with wget,<br>they need an automated browser that will run the JavaScript to make the pages come to life.</p>\n</div>\n<div class=\"section\">\n  <hr>\n</div>\n<div class=\"section\">\n  <p>When we are inventing on the command line,<br>we have to jump ahead of ourselves and see what our program will need when it is finished.</p>\n  <p>And the first thing that comes to mind,<br>is multiple news sources.</p>\n  <p>If we are to create a system to extract interesting stories from multiple places on the internet,<br>then it is definable something that should consist of multiple programs, that run hourly or so.</p>\n  <p>So we would have one program for extracting data from hacker news,<br>another for extracting data from eBay about some obscure phone that is cool but never available.</p>\n  <p>And another program to get emails from different service providers,<br>news from, say, the Raspberry PI blog.</p>\n  <p>And information from Raspberry Pi, like temperature or pressure changes,<br>maybe a new hires photos from the nearby Squirrel cam.</p>\n</div>\n<div class=\"section\">\n  <hr>\n</div>\n<div class=\"section\">\n  <p>We could make this work via a command line RSS reader (with the option of using a proper desktop one),<br>by setting up a local RSS server that keeps track of data from all these places.</p>\n  <p>And I think the data would have to sit in an SQL database,<br>this way all those different data sources would have a universal format that can be easily processed.</p>\n  <p>Once they are put into the database,<br>creating custom RSS feeds is easy, they are actually just plain XML files, same idea as JSON.</p>\n</div>\n<div class=\"section\">\n  <hr>\n</div>\n<div class=\"section\">\n  <p>While SQL and the feed server can be easily can be easily controlled from the command line,<br>the scraping of the various unfriendly websites, will require a custom program.</p>\n  <p>Personally I would base it on <a href=\"https://browserless.js.org/\">Browserless.js</a>,<br>as it has built-in \"evasions technniques (sic)\" to prevent being blocked.</p>\n  <p>I just noticed that they misspelled evasion techniques,<br>forked their project, and fixed a couple more mistakes I could find.</p>\n  <p><a href=\"https://github.com/microlinkhq/browserless/pull/295\">I created a PR, short for pull request, which means that I am requesting that they pull the fixes that I have created</a>,<br>so because Browserless.js is an Open Source Code Project, anybody can make valuable little contributions.</p>\n  <p>But even with the mighty Browserless.js project,<br>websites we would be scraping will sometimes update their code, and our scraper brains would need to be updated.</p>\n</div>\n<div class=\"section\">\n  <hr>\n</div>\n<div class=\"section\">\n  <p>Overall, our best option here, is to cut 10 entries from top of the Hacker News,<br>and see if we can get away with making 10 quick requests for their top stories, and call our initial version done.</p>\n  <p>Personally I would begin testing <a href=\"https://ostechnix.com/newsbeuter-command-line-rssatom-feed-reader-unix-like-systems/\">Newsbeuter</a> <a href=\"https://github.com/msharov/snownews\">Snownews</a> <a href=\"https://www.tecmint.com/newsroom-commandline-linux-news-reader/\">Newsroom</a> and <a href=\"https://www.tecmint.com/newsboat-rss-atom-feed-reader-for-linux-terminals/\">newsboat</a> to get the feel for command line news,<br>and consider moving to desktop RSS clients so as long as they support ad blockers.</p>\n  <p>And then quietly, I would begin working on my scrapers,<br>being mindful that they must output RSS feeds, which then I would add to my RSS readers.</p>\n</div>\n<div class=\"section\">\n  <hr>\n</div>\n<div class=\"section\">\n  <p>To be honest, there is not much time that I have for reading internet news,<br>and the more I think about it, the more fetching that little 10 news items program seems, and the more clunky RSS starts to look.</p>\n  <p>Let us chop off the top then entries from Hacker News,<br>and see if we can quickly get an overview of what is going on in the world.</p>\n  <p>The command for getting the top of data is head, and to specify how many lines we want we use -n,<br>so our command now looks like this: curl <a href=\"https://hacker-news.firebaseio.com/v0/topstories.json?print=pretty\">https://hacker-news.firebaseio.com/v0/topstories.json?print=pretty</a> | jq '.[]' | head -n 10;</p>\n  <p>Armed with 10 numbers that represent top stories,<br>we now have to take the second step, and use each of those numbers to download the titles from Hacker News.</p>\n  <p>And here we reuse curl, but the url that we give to curl needs to have that number inserted in there,<br>there are two ways to do this, one is xargs, and the other a while loop.</p>\n  <p>The while loop is better, it is more readable, it is faster, but I don't like it because it wraps things,<br>my xargs approach, creates a single straight command line.</p>\n  <p>Unfortunately my line ends up having a command that uses quotes, inside quotes,<br>so there is quote escaping, by means of backslash, and that is something I don't like.</p>\n  <p>But I must say the proper way to do this, is a single pipe,<br>get the top story numbers, map the numbers to data, and map the data to text so that it looks nice on the black screen.</p>\n  <p>Three simple steps,<br>get numbers, convert them to stories, print them to screen.</p>\n  <p>In closing, I will read to you the code of each of the three approaches, in the way that I think about it,<br>I wont read quote, backslash, n, backslash n, but give you a somewhat poetic reading.</p>\n</div>\n<div class=\"section\">\n  <hr>\n</div>\n<div class=\"section\">\n  <p>First is my xargs approach, a single pipe;</p>\n  <p>curl -s <code>https://hacker-news.firebaseio.com/v0/topstories.json</code> | jq '.[]' | head -n 10 | xargs -I % sh -c \"curl -s <code>https://hacker-news.firebaseio.com/v0/item/%.json</code> | jq -r '. | [.title,.url, null] | join(\"\\n\")'\"</p>\n  <p>Now, describe the first loop approach, which is actually a little bit backwards.</p>\n  <p>while read -r ID; do curl -s <code>https://hacker-news.firebaseio.com/v0/item/$ID.json</code> | jq -r '. | [.title, \"\\n\", .url, \"\\n\"] | add'; done &lt; &lt;(curl -s <code>https://hacker-news.firebaseio.com/v0/topstories.json</code> | jq '.[]' | head -n 10 )</p>\n  <p>And here is the way that people probably prefer, but I am unhappy about, because there shoudn't be a loop here.</p>\n  <p>curl -s <code>https://hacker-news.firebaseio.com/v0/topstories.json</code> | jq '.[]' | head -n 10 | while read -r ID ; do curl -s <code>https://hacker-news.firebaseio.com/v0/item/$ID.json</code> | jq -r '. | [.title, \"\\n\", .url, \"\\n\"] | add'; done;</p>\n  <p>And that is what the black screens are all about,<br>I hope I inspired you to look a little bit more into those black screens.</p>\n</div>",
  "bootstrap": "<div class=\"card card-section bg-dark text-warning shadow\">\n  <div class=\"section card-body mb-0 my-2\">\n    <div class=\"paragraph card-text card-stanza my-5 text-center\">The shortest possible answer is,<br><a href=\"https://linuxcommandlibrary.com/basic/oneliners\">https://linuxcommandlibrary.com/basic/oneliners</a> to get a good overview of what they are typing.</div>\n    <div class=\"paragraph card-text card-stanza my-5 text-center\">It is a different kind of a user interface,<br>and the applications within that world can be connected together to make new applications.</div>\n    <div class=\"paragraph card-text card-stanza my-5 text-center\">This used to be the original computer interface,<br>before graphic applications and the mouse were invented.</div>\n    <div class=\"paragraph card-text card-stanza my-5 text-center\">Today it is used because it is faster than the GUI,<br>it is less distracting, and it is easier to make those programs.</div>\n  </div>\n</div>\n<div class=\"mb-5 section-spacer\">&nbsp;</div>\n<div class=\"card card-section bg-dark text-warning shadow\">\n  <div class=\"section card-body mb-0 my-2\">\n    <div class=\"paragraph card-text card-stanza my-5 text-center\">Think about playing music, how many icons you have to click to get there,<br>on the command line you just type in <code>cvlc ~/Music/*</code> and keep doing what you were doing.</div>\n    <div class=\"paragraph card-text card-stanza my-5 text-center\">The reason why graphic user interfaces are popular,<br>is because they have an easy on ramp, they give you breadcrumbs.</div>\n    <div class=\"paragraph card-text card-stanza my-5 text-center\">The command line requires that you remember where your files are,<br>and remember some of the command names, such as cvlc to play music, or wget to download a file.</div>\n  </div>\n</div>\n<div class=\"mb-5 section-spacer\">&nbsp;</div>\n<div class=\"card card-section bg-dark text-warning shadow\">\n  <div class=\"section card-body mb-0 my-2\">\n    <div class=\"paragraph card-text card-stanza my-5 text-center\">You don't have to memorize those commands,<br><a href=\"https://www.google.com/search?q=Linux+Cheat+Sheet\">you can just download a little cheat-sheet</a>.</div>\n    <div class=\"paragraph card-text card-stanza my-5 text-center\">After a while you will stop forgetting their names,<br>it is really not that big of a deal.</div>\n    <div class=\"paragraph card-text card-stanza my-5 text-center\">And if you need to browse some photos,<br>then you just use a normal GUI program that can display them.</div>\n    <div class=\"paragraph card-text card-stanza my-5 text-center\">Though, instead of clicking on an icon somewhere,<br>you'll launch it from the command line by entering its name.</div>\n    <div class=\"paragraph card-text card-stanza my-5 text-center\">It is just another command to add to your cheat sheet,<br>and just like there is a limited number of things that you do on your phone.</div>\n    <div class=\"paragraph card-text card-stanza my-5 text-center\">You'll also limit yourself to just knowing 5 to 20 things,<br>that you normally do on the command line.</div>\n  </div>\n</div>\n<div class=\"mb-5 section-spacer\">&nbsp;</div>\n<div class=\"card card-section bg-dark text-warning shadow\">\n  <div class=\"section card-body mb-0 my-2\">\n    <div class=\"paragraph card-text card-stanza my-5 text-center\">Just like your phone apps or desktop applications,<br>programs on the command line can do multiple things.</div>\n    <div class=\"paragraph card-text card-stanza my-5 text-center\">every command has a help feature,<br>where you type in the command name followed by <code>-h</code> and that will list all the different things that a command can do.</div>\n    <div class=\"paragraph card-text card-stanza my-5 text-center\">For example <a href=\"https://www.guyrutenberg.com/2014/05/02/make-offline-mirror-of-a-site-using-wget/\">wget has a --mirror option</a>,<br>here you don't just download a file, but mirror the entire website.</div>\n    <div class=\"paragraph card-text card-stanza my-5 text-center\">So just imagine wget being an app on your phone, called Web Get,<br>and it would have two buttons, \"Download A File\", and \"Download Website\".</div>\n  </div>\n</div>\n<div class=\"mb-5 section-spacer\">&nbsp;</div>\n<div class=\"card card-section bg-dark text-warning shadow\">\n  <div class=\"section card-body mb-0 my-2\">\n    <div class=\"paragraph card-text card-stanza my-5 text-center\">Now let me give you a more involved example,<br>I would like to show you how to download internet stories.</div>\n    <div class=\"paragraph card-text card-stanza my-5 text-center\">Every website offers some way of getting some of their data,<br>Hacker News (a website for programmers) has a nice description of their raw data: <a href=\"https://github.com/HackerNews/API\">https://github.com/HackerNews/API</a></div>\n    <div class=\"paragraph card-text card-stanza my-5 text-center\">We learn that their website data structure is all about numbers,<br>and to get at those number we have to download their <a href=\"https://hacker-news.firebaseio.com/v0/topstories.json?print=pretty\">topstories.json</a></div>\n    <div class=\"paragraph card-text card-stanza my-5 text-center\">JSON is a file format, it is not really purrfect for the command line,<br>so we will thin it, reduce it simplify with a program called <a href=\"https://stedolan.github.io/jq/\">jq</a>,</div>\n    <div class=\"paragraph card-text card-stanza my-5 text-center\">jq reaches into the JSON file format and grabs plain text for us,<br>without the extra curly bracets, quotes, or in this case, commas that delimit the entries and square brackets that represent lists of data.</div>\n    <div class=\"paragraph card-text card-stanza my-5 text-center\">We will use <a href=\"https://www.youtube.com/watch?v=I6id1Y0YuNk\">curl</a>, it is very similar to wget,<br>but it doesn't have that fancy extra stuff for mirroring websites.</div>\n    <div class=\"paragraph card-text card-stanza my-5 text-center\">And we need a <a href=\"https://stedolan.github.io/jq/manual/#Basicfilters\">jq tutorial</a> to give us some hints on how to use it, <a href=\"https://www.youtube.com/watch?v=FSn_38gDvzM\">sometimes watching a video tutorial is useful too</a>,<br>I found the stuff under \"Array/Object Value Iterator: .[]\".</div>\n    <div class=\"paragraph card-text card-stanza my-5 text-center\">Programmers speak like this when they assume only other programmers will read their stuff,<br>little do they know, we don't really read their funk, we just look at the code.</div>\n  </div>\n</div>\n<div class=\"mb-5 section-spacer\">&nbsp;</div>\n<div class=\"card card-section bg-dark text-warning shadow\">\n  <div class=\"section card-body mb-0 my-2\">\n    <div class=\"paragraph card-text card-stanza my-5 text-center\">That is an important lesson, never read the freaking manual,<br>what takes you 20 minutes instead of 1 minute, teaches you 20 times more.</div>\n    <div class=\"paragraph card-text card-stanza my-5 text-center\">There is a lot of people out there that say <a href=\"http://www.catb.org/~esr/jargon/html/R/RTFM.html\">RTFM</a>,<br>but that is only because they don't know the answer, because they spent their career brainlessly looking everything up - just ignore them.</div>\n  </div>\n</div>\n<div class=\"mb-5 section-spacer\">&nbsp;</div>\n<div class=\"card card-section bg-dark text-warning shadow\">\n  <div class=\"section card-body mb-0 my-2\">\n    <div class=\"paragraph card-text card-stanza my-5 text-center\">So to get at the numbers we have to say<br>curl <a href=\"https://hacker-news.firebaseio.com/v0/topstories.json?print=pretty\">https://hacker-news.firebaseio.com/v0/topstories.json?print=pretty</a> | jq '.[]'.</div>\n    <div class=\"paragraph card-text card-stanza my-5 text-center\">Those single quotes tell your command line that your code in there belong to jq,<br>the command line is a programming language it self, to keep command related stuff isolated we use single brackets.</div>\n  </div>\n</div>\n<div class=\"mb-5 section-spacer\">&nbsp;</div>\n<div class=\"card card-section bg-dark text-warning shadow\">\n  <div class=\"section card-body mb-0 my-2\">\n    <div class=\"paragraph card-text card-stanza my-5 text-center\">The way you get a feel for this is by looking at <a href=\"https://www.google.com/search?q=command+line+cheat+sheets\">command line cheat sheets</a>,<br>and <a href=\"https://www.youtube.com/results?search_query=Linux+command+line+tutorial\">video tutorials</a>, and if you are going somewhere without computers, <a href=\"https://www.google.com/search?q=linux+pocket+reference\">grab some pocket reference titles</a>.</div>\n    <div class=\"paragraph card-text card-stanza my-5 text-center\">The smaller the source of information,<br>the more valuable the information within.</div>\n    <div class=\"paragraph card-text card-stanza my-5 text-center\">If you buy a Linux Bible,<br>you won't be able to tell what is important and what isn't.</div>\n    <div class=\"paragraph card-text card-stanza my-5 text-center\">And it is really cool to just have a long little row,<br>of pocket reference books, it shows that you don't mess around.</div>\n  </div>\n</div>\n<div class=\"mb-5 section-spacer\">&nbsp;</div>\n<div class=\"card card-section bg-dark text-warning shadow\">\n  <div class=\"section card-body mb-0 my-2\">\n    <div class=\"paragraph card-text card-stanza my-5 text-center\">At this point we have a lit of numbers that represent stories on hacker news,<br>to grab a story we have to curl item / story number followed by .json <a href=\"https://hacker-news.firebaseio.com/v0/item/27941616.json?print=pretty\">27941616.json</a></div>\n    <div class=\"paragraph card-text card-stanza my-5 text-center\">There is a bit of a problem here, because piping the output of the list of numbers to,<br>wc command with the -l switch, which stands for word count, count number of lines for me.</div>\n    <div class=\"paragraph card-text card-stanza my-5 text-center\">Reveals that hacker news API returns 500 story numbers,<br>and it is bad etiquette to issue 500 requests to hacker news servers.</div>\n    <div class=\"paragraph card-text card-stanza my-5 text-center\">Moreover, each hacker news story has, a list of numbers that represent the comments,<br>presumably the top comments in the thread.</div>\n    <div class=\"paragraph card-text card-stanza my-5 text-center\">Even though they say there is no rate limit,<br>we would probably trigger their security system and get temporary banned.</div>\n  </div>\n</div>\n<div class=\"mb-5 section-spacer\">&nbsp;</div>\n<div class=\"card card-section bg-dark text-warning shadow\">\n  <div class=\"section card-body mb-0 my-2\">\n    <div class=\"paragraph card-text card-stanza my-5 text-center\">I recently got banned by npm becasue i was looking for package names I could use to publish a program,<br>I knew I wanted something similar to rsync, which is the name of a command responsible for remote synchronization.</div>\n    <div class=\"paragraph card-text card-stanza my-5 text-center\">But somebody else took rsync ages ago, so I started changing the url in the address bar to<br>ssync tsync usync vsync wsync xsync ysync,</div>\n    <div class=\"paragraph card-text card-stanza my-5 text-center\">And all of a sudden the website said, you are doing too much of that,<br>you are temporary banned - fair enough, I guess.</div>\n    <div class=\"paragraph card-text card-stanza my-5 text-center\">I ended up publishing <a href=\"https://github.com/catpea/rsend\">rsend</a>, because it occurred to me,<br>that I am not really syncing arbitrary locations, but sending from local to remote.</div>\n    <div class=\"paragraph card-text card-stanza my-5 text-center\">I use it, to upload my music to my workout audio player,<br>but the program is very ugly because, I only expect GNU utilities on the target system.</div>\n    <div class=\"paragraph card-text card-stanza my-5 text-center\">While I can program all kinds of things on the local system,<br>the remote system will not have my code, so checking file uploads has to be done via Linux commands.</div>\n  </div>\n</div>\n<div class=\"mb-5 section-spacer\">&nbsp;</div>\n<div class=\"card card-section bg-dark text-warning shadow\">\n  <div class=\"section card-body mb-0 my-2\">\n    <div class=\"paragraph card-text card-stanza my-5 text-center\">One way to throttle our requests, is to use the sleep command, between downloading each of the stories,<br>we could sleep for 1 second for the first 10 stories, and then, two seconds, three seconds, and finally five seconds between each request, and nobody would get mad a that.</div>\n    <div class=\"paragraph card-text card-stanza my-5 text-center\">Another way to do it, is to pre-fetch three or five stories,<br>ahead of where you are at right now, so you wouldn't have any wait times, if you just read the stories live.</div>\n    <div class=\"paragraph card-text card-stanza my-5 text-center\">If all you want to see for the day is 10 stories,<br>then you will only download 15, AND NOT 500!</div>\n    <div class=\"paragraph card-text card-stanza my-5 text-center\">Beyond that, you could do a little bit of maneuvering,<br>and begin gathering top stories every hour throughout the day at a comfy 10 second interval.</div>\n  </div>\n</div>\n<div class=\"mb-5 section-spacer\">&nbsp;</div>\n<div class=\"card card-section bg-dark text-warning shadow\">\n  <div class=\"section card-body mb-0 my-2\">\n    <div class=\"paragraph card-text card-stanza my-5 text-center\">There is something you should know,<br>you know how that wc command that we just rand had a -l to make it couldn't lines.</div>\n    <div class=\"paragraph card-text card-stanza my-5 text-center\">When you are inside a full screen program,<br>interacting with things kind of should be maybe about letters as well.</div>\n    <div class=\"paragraph card-text card-stanza my-5 text-center\">If we made an interactive terminal program,<br>that just browsers a screenful at a time.</div>\n    <div class=\"paragraph card-text card-stanza my-5 text-center\">To go to the next page,<br>you would have to pres the \"n\" key.</div>\n    <div class=\"paragraph card-text card-stanza my-5 text-center\">This is yet another Black Screen shortcut,<br>that goes straight into a cheeatsheet.</div>\n    <div class=\"paragraph card-text card-stanza my-5 text-center\">We also need to consider other news sources,<br>whatever your favorite hangout spots are.</div>\n    <div class=\"paragraph card-text card-stanza my-5 text-center\">And, not everyone is going to be as nice as Hacker News and present an API,<br>there may not be a public API.</div>\n    <div class=\"paragraph card-text card-stanza my-5 text-center\">In that case we will need to write a screen scraper,<br>that is one of the worst kinds of programs.</div>\n    <div class=\"paragraph card-text card-stanza my-5 text-center\">As it doesn't use the data beneath the screen,<br>it just sort of does this ugly scrape of whatever is on the screen.</div>\n    <div class=\"paragraph card-text card-stanza my-5 text-center\">This entails controlling a full Web Browser,<br>and sort of making it look like there is a human browsing the page, where it is actually a freaking robot.</div>\n    <div class=\"paragraph card-text card-stanza my-5 text-center\">And it gets worse,<br>lyke super bad.</div>\n    <div class=\"paragraph card-text card-stanza my-5 text-center\">There was a world wide DNS problem couple of days ago,<br>and Amazon keeps forcing me to solve their CAPTCHA when I use Firefox, Chromium works fine for me.</div>\n    <div class=\"paragraph card-text card-stanza my-5 text-center\">There maybe websites out there that can stop you from scraping them,<br>by presenting a CAPTCHA, and the only way to fix that, is to solve it.</div>\n    <div class=\"paragraph card-text card-stanza my-5 text-center\">The glitched Amazon that I am still experiencing,<br>would only ask me to solve the CAPTCHA once per opening a browser, and that was already pretty annoying.</div>\n    <div class=\"paragraph card-text card-stanza my-5 text-center\">CAPTCHA is unlikely to feature on News Websites though,<br>but if you scrape too quickly, of they detect your browser signature you may bet one.</div>\n    <div class=\"paragraph card-text card-stanza my-5 text-center\">Now that all websites use JavaScript, you can't just mirror them with wget,<br>they need an automated browser that will run the JavaScript to make the pages come to life.</div>\n  </div>\n</div>\n<div class=\"mb-5 section-spacer\">&nbsp;</div>\n<div class=\"card card-section bg-dark text-warning shadow\">\n  <div class=\"section card-body mb-0 my-2\">\n    <div class=\"paragraph card-text card-stanza my-5 text-center\">When we are inventing on the command line,<br>we have to jump ahead of ourselves and see what our program will need when it is finished.</div>\n    <div class=\"paragraph card-text card-stanza my-5 text-center\">And the first thing that comes to mind,<br>is multiple news sources.</div>\n    <div class=\"paragraph card-text card-stanza my-5 text-center\">If we are to create a system to extract interesting stories from multiple places on the internet,<br>then it is definable something that should consist of multiple programs, that run hourly or so.</div>\n    <div class=\"paragraph card-text card-stanza my-5 text-center\">So we would have one program for extracting data from hacker news,<br>another for extracting data from eBay about some obscure phone that is cool but never available.</div>\n    <div class=\"paragraph card-text card-stanza my-5 text-center\">And another program to get emails from different service providers,<br>news from, say, the Raspberry PI blog.</div>\n    <div class=\"paragraph card-text card-stanza my-5 text-center\">And information from Raspberry Pi, like temperature or pressure changes,<br>maybe a new hires photos from the nearby Squirrel cam.</div>\n  </div>\n</div>\n<div class=\"mb-5 section-spacer\">&nbsp;</div>\n<div class=\"card card-section bg-dark text-warning shadow\">\n  <div class=\"section card-body mb-0 my-2\">\n    <div class=\"paragraph card-text card-stanza my-5 text-center\">We could make this work via a command line RSS reader (with the option of using a proper desktop one),<br>by setting up a local RSS server that keeps track of data from all these places.</div>\n    <div class=\"paragraph card-text card-stanza my-5 text-center\">And I think the data would have to sit in an SQL database,<br>this way all those different data sources would have a universal format that can be easily processed.</div>\n    <div class=\"paragraph card-text card-stanza my-5 text-center\">Once they are put into the database,<br>creating custom RSS feeds is easy, they are actually just plain XML files, same idea as JSON.</div>\n  </div>\n</div>\n<div class=\"mb-5 section-spacer\">&nbsp;</div>\n<div class=\"card card-section bg-dark text-warning shadow\">\n  <div class=\"section card-body mb-0 my-2\">\n    <div class=\"paragraph card-text card-stanza my-5 text-center\">While SQL and the feed server can be easily can be easily controlled from the command line,<br>the scraping of the various unfriendly websites, will require a custom program.</div>\n    <div class=\"paragraph card-text card-stanza my-5 text-center\">Personally I would base it on <a href=\"https://browserless.js.org/\">Browserless.js</a>,<br>as it has built-in \"evasions technniques (sic)\" to prevent being blocked.</div>\n    <div class=\"paragraph card-text card-stanza my-5 text-center\">I just noticed that they misspelled evasion techniques,<br>forked their project, and fixed a couple more mistakes I could find.</div>\n    <div class=\"paragraph card-text card-stanza my-5 text-center\"><a href=\"https://github.com/microlinkhq/browserless/pull/295\">I created a PR, short for pull request, which means that I am requesting that they pull the fixes that I have created</a>,<br>so because Browserless.js is an Open Source Code Project, anybody can make valuable little contributions.</div>\n    <div class=\"paragraph card-text card-stanza my-5 text-center\">But even with the mighty Browserless.js project,<br>websites we would be scraping will sometimes update their code, and our scraper brains would need to be updated.</div>\n  </div>\n</div>\n<div class=\"mb-5 section-spacer\">&nbsp;</div>\n<div class=\"card card-section bg-dark text-warning shadow\">\n  <div class=\"section card-body mb-0 my-2\">\n    <div class=\"paragraph card-text card-stanza my-5 text-center\">Overall, our best option here, is to cut 10 entries from top of the Hacker News,<br>and see if we can get away with making 10 quick requests for their top stories, and call our initial version done.</div>\n    <div class=\"paragraph card-text card-stanza my-5 text-center\">Personally I would begin testing <a href=\"https://ostechnix.com/newsbeuter-command-line-rssatom-feed-reader-unix-like-systems/\">Newsbeuter</a> <a href=\"https://github.com/msharov/snownews\">Snownews</a> <a href=\"https://www.tecmint.com/newsroom-commandline-linux-news-reader/\">Newsroom</a> and <a href=\"https://www.tecmint.com/newsboat-rss-atom-feed-reader-for-linux-terminals/\">newsboat</a> to get the feel for command line news,<br>and consider moving to desktop RSS clients so as long as they support ad blockers.</div>\n    <div class=\"paragraph card-text card-stanza my-5 text-center\">And then quietly, I would begin working on my scrapers,<br>being mindful that they must output RSS feeds, which then I would add to my RSS readers.</div>\n  </div>\n</div>\n<div class=\"mb-5 section-spacer\">&nbsp;</div>\n<div class=\"card card-section bg-dark text-warning shadow\">\n  <div class=\"section card-body mb-0 my-2\">\n    <div class=\"paragraph card-text card-stanza my-5 text-center\">To be honest, there is not much time that I have for reading internet news,<br>and the more I think about it, the more fetching that little 10 news items program seems, and the more clunky RSS starts to look.</div>\n    <div class=\"paragraph card-text card-stanza my-5 text-center\">Let us chop off the top then entries from Hacker News,<br>and see if we can quickly get an overview of what is going on in the world.</div>\n    <div class=\"paragraph card-text card-stanza my-5 text-center\">The command for getting the top of data is head, and to specify how many lines we want we use -n,<br>so our command now looks like this: curl <a href=\"https://hacker-news.firebaseio.com/v0/topstories.json?print=pretty\">https://hacker-news.firebaseio.com/v0/topstories.json?print=pretty</a> | jq '.[]' | head -n 10;</div>\n    <div class=\"paragraph card-text card-stanza my-5 text-center\">Armed with 10 numbers that represent top stories,<br>we now have to take the second step, and use each of those numbers to download the titles from Hacker News.</div>\n    <div class=\"paragraph card-text card-stanza my-5 text-center\">And here we reuse curl, but the url that we give to curl needs to have that number inserted in there,<br>there are two ways to do this, one is xargs, and the other a while loop.</div>\n    <div class=\"paragraph card-text card-stanza my-5 text-center\">The while loop is better, it is more readable, it is faster, but I don't like it because it wraps things,<br>my xargs approach, creates a single straight command line.</div>\n    <div class=\"paragraph card-text card-stanza my-5 text-center\">Unfortunately my line ends up having a command that uses quotes, inside quotes,<br>so there is quote escaping, by means of backslash, and that is something I don't like.</div>\n    <div class=\"paragraph card-text card-stanza my-5 text-center\">But I must say the proper way to do this, is a single pipe,<br>get the top story numbers, map the numbers to data, and map the data to text so that it looks nice on the black screen.</div>\n    <div class=\"paragraph card-text card-stanza my-5 text-center\">Three simple steps,<br>get numbers, convert them to stories, print them to screen.</div>\n    <div class=\"paragraph card-text card-stanza my-5 text-center\">In closing, I will read to you the code of each of the three approaches, in the way that I think about it,<br>I wont read quote, backslash, n, backslash n, but give you a somewhat poetic reading.</div>\n  </div>\n</div>\n<div class=\"mb-5 section-spacer\">&nbsp;</div>\n<div class=\"card card-section bg-dark text-warning shadow\">\n  <div class=\"section card-body mb-0 my-2\">\n    <div class=\"paragraph card-text card-stanza my-5 text-center\">First is my xargs approach, a single pipe;</div>\n    <div class=\"paragraph card-text card-stanza my-5 text-center\">curl -s <code>https://hacker-news.firebaseio.com/v0/topstories.json</code> | jq '.[]' | head -n 10 | xargs -I % sh -c \"curl -s <code>https://hacker-news.firebaseio.com/v0/item/%.json</code> | jq -r '. | [.title,.url, null] | join(\"\\n\")'\"</div>\n    <div class=\"paragraph card-text card-stanza my-5 text-center\">Now, describe the first loop approach, which is actually a little bit backwards.</div>\n    <div class=\"paragraph card-text card-stanza my-5 text-center\">while read -r ID; do curl -s <code>https://hacker-news.firebaseio.com/v0/item/$ID.json</code> | jq -r '. | [.title, \"\\n\", .url, \"\\n\"] | add'; done &lt; &lt;(curl -s <code>https://hacker-news.firebaseio.com/v0/topstories.json</code> | jq '.[]' | head -n 10 )</div>\n    <div class=\"paragraph card-text card-stanza my-5 text-center\">And here is the way that people probably prefer, but I am unhappy about, because there shoudn't be a loop here.</div>\n    <div class=\"paragraph card-text card-stanza my-5 text-center\">curl -s <code>https://hacker-news.firebaseio.com/v0/topstories.json</code> | jq '.[]' | head -n 10 | while read -r ID ; do curl -s <code>https://hacker-news.firebaseio.com/v0/item/$ID.json</code> | jq -r '. | [.title, \"\\n\", .url, \"\\n\"] | add'; done;</div>\n    <div class=\"paragraph card-text card-stanza my-5 text-center\">And that is what the black screens are all about,<br>I hope I inspired you to look a little bit more into those black screens.</div>\n  </div>\n</div>",
  "print": "<div class=\"section avoid-break-inside\" style=\"padding-bottom: 2rem;\">\n  <div class=\"paragraph\">The shortest possible answer is,<br><span>https://linuxcommandlibrary.com/basic/oneliners<sup>[1]</sup></span> to get a good overview of what they are typing.</div>\n  <div class=\"paragraph\">It is a different kind of a user interface,<br>and the applications within that world can be connected together to make new applications.</div>\n  <div class=\"paragraph\">This used to be the original computer interface,<br>before graphic applications and the mouse were invented.</div>\n  <div class=\"paragraph\">Today it is used because it is faster than the GUI,<br>it is less distracting, and it is easier to make those programs.</div>\n</div>\n<div class=\"section avoid-break-inside\" style=\"padding-bottom: 2rem;\">\n  <br>\n</div>\n<div class=\"section avoid-break-inside\" style=\"padding-bottom: 2rem;\">\n  <div class=\"paragraph\">Think about playing music, how many icons you have to click to get there,<br>on the command line you just type in <code>cvlc ~/Music/*</code> and keep doing what you were doing.</div>\n  <div class=\"paragraph\">The reason why graphic user interfaces are popular,<br>is because they have an easy on ramp, they give you breadcrumbs.</div>\n  <div class=\"paragraph\">The command line requires that you remember where your files are,<br>and remember some of the command names, such as cvlc to play music, or wget to download a file.</div>\n</div>\n<div class=\"section avoid-break-inside\" style=\"padding-bottom: 2rem;\">\n  <br>\n</div>\n<div class=\"section avoid-break-inside\" style=\"padding-bottom: 2rem;\">\n  <div class=\"paragraph\">You don't have to memorize those commands,<br><span>you can just download a little cheat-sheet<sup>[2]</sup></span>.</div>\n  <div class=\"paragraph\">After a while you will stop forgetting their names,<br>it is really not that big of a deal.</div>\n  <div class=\"paragraph\">And if you need to browse some photos,<br>then you just use a normal GUI program that can display them.</div>\n  <div class=\"paragraph\">Though, instead of clicking on an icon somewhere,<br>you'll launch it from the command line by entering its name.</div>\n  <div class=\"paragraph\">It is just another command to add to your cheat sheet,<br>and just like there is a limited number of things that you do on your phone.</div>\n  <div class=\"paragraph\">You'll also limit yourself to just knowing 5 to 20 things,<br>that you normally do on the command line.</div>\n</div>\n<div class=\"section avoid-break-inside\" style=\"padding-bottom: 2rem;\">\n  <br>\n</div>\n<div class=\"section avoid-break-inside\" style=\"padding-bottom: 2rem;\">\n  <div class=\"paragraph\">Just like your phone apps or desktop applications,<br>programs on the command line can do multiple things.</div>\n  <div class=\"paragraph\">every command has a help feature,<br>where you type in the command name followed by <code>-h</code> and that will list all the different things that a command can do.</div>\n  <div class=\"paragraph\">For example <span>wget has a --mirror option<sup>[3]</sup></span>,<br>here you don't just download a file, but mirror the entire website.</div>\n  <div class=\"paragraph\">So just imagine wget being an app on your phone, called Web Get,<br>and it would have two buttons, \"Download A File\", and \"Download Website\".</div>\n</div>\n<div class=\"section avoid-break-inside\" style=\"padding-bottom: 2rem;\">\n  <br>\n</div>\n<div class=\"section avoid-break-inside\" style=\"padding-bottom: 2rem;\">\n  <div class=\"paragraph\">Now let me give you a more involved example,<br>I would like to show you how to download internet stories.</div>\n  <div class=\"paragraph\">Every website offers some way of getting some of their data,<br>Hacker News (a website for programmers) has a nice description of their raw data: <span>https://github.com/HackerNews/API<sup>[4]</sup></span></div>\n  <div class=\"paragraph\">We learn that their website data structure is all about numbers,<br>and to get at those number we have to download their <span>topstories.json<sup>[5]</sup></span></div>\n  <div class=\"paragraph\">JSON is a file format, it is not really purrfect for the command line,<br>so we will thin it, reduce it simplify with a program called <span>jq<sup>[6]</sup></span>,</div>\n  <div class=\"paragraph\">jq reaches into the JSON file format and grabs plain text for us,<br>without the extra curly bracets, quotes, or in this case, commas that delimit the entries and square brackets that represent lists of data.</div>\n  <div class=\"paragraph\">We will use <span>curl<sup>[7]</sup></span>, it is very similar to wget,<br>but it doesn't have that fancy extra stuff for mirroring websites.</div>\n  <div class=\"paragraph\">And we need a <span>jq tutorial<sup>[8]</sup></span> to give us some hints on how to use it, <span>sometimes watching a video tutorial is useful too<sup>[9]</sup></span>,<br>I found the stuff under \"Array/Object Value Iterator: .[]\".</div>\n  <div class=\"paragraph\">Programmers speak like this when they assume only other programmers will read their stuff,<br>little do they know, we don't really read their funk, we just look at the code.</div>\n</div>\n<div class=\"section avoid-break-inside\" style=\"padding-bottom: 2rem;\">\n  <br>\n</div>\n<div class=\"section avoid-break-inside\" style=\"padding-bottom: 2rem;\">\n  <div class=\"paragraph\">That is an important lesson, never read the freaking manual,<br>what takes you 20 minutes instead of 1 minute, teaches you 20 times more.</div>\n  <div class=\"paragraph\">There is a lot of people out there that say <span>RTFM<sup>[10]</sup></span>,<br>but that is only because they don't know the answer, because they spent their career brainlessly looking everything up - just ignore them.</div>\n</div>\n<div class=\"section avoid-break-inside\" style=\"padding-bottom: 2rem;\">\n  <br>\n</div>\n<div class=\"section avoid-break-inside\" style=\"padding-bottom: 2rem;\">\n  <div class=\"paragraph\">So to get at the numbers we have to say<br>curl <span>https://hacker-news.firebaseio.com/v0/topstories.json?print=pretty<sup>[11]</sup></span> | jq '.[]'.</div>\n  <div class=\"paragraph\">Those single quotes tell your command line that your code in there belong to jq,<br>the command line is a programming language it self, to keep command related stuff isolated we use single brackets.</div>\n</div>\n<div class=\"section avoid-break-inside\" style=\"padding-bottom: 2rem;\">\n  <br>\n</div>\n<div class=\"section avoid-break-inside\" style=\"padding-bottom: 2rem;\">\n  <div class=\"paragraph\">The way you get a feel for this is by looking at <span>command line cheat sheets<sup>[12]</sup></span>,<br>and <span>video tutorials<sup>[13]</sup></span>, and if you are going somewhere without computers, <span>grab some pocket reference titles<sup>[14]</sup></span>.</div>\n  <div class=\"paragraph\">The smaller the source of information,<br>the more valuable the information within.</div>\n  <div class=\"paragraph\">If you buy a Linux Bible,<br>you won't be able to tell what is important and what isn't.</div>\n  <div class=\"paragraph\">And it is really cool to just have a long little row,<br>of pocket reference books, it shows that you don't mess around.</div>\n</div>\n<div class=\"section avoid-break-inside\" style=\"padding-bottom: 2rem;\">\n  <br>\n</div>\n<div class=\"section avoid-break-inside\" style=\"padding-bottom: 2rem;\">\n  <div class=\"paragraph\">At this point we have a lit of numbers that represent stories on hacker news,<br>to grab a story we have to curl item / story number followed by .json <span>27941616.json<sup>[15]</sup></span></div>\n  <div class=\"paragraph\">There is a bit of a problem here, because piping the output of the list of numbers to,<br>wc command with the -l switch, which stands for word count, count number of lines for me.</div>\n  <div class=\"paragraph\">Reveals that hacker news API returns 500 story numbers,<br>and it is bad etiquette to issue 500 requests to hacker news servers.</div>\n  <div class=\"paragraph\">Moreover, each hacker news story has, a list of numbers that represent the comments,<br>presumably the top comments in the thread.</div>\n  <div class=\"paragraph\">Even though they say there is no rate limit,<br>we would probably trigger their security system and get temporary banned.</div>\n</div>\n<div class=\"section avoid-break-inside\" style=\"padding-bottom: 2rem;\">\n  <br>\n</div>\n<div class=\"section avoid-break-inside\" style=\"padding-bottom: 2rem;\">\n  <div class=\"paragraph\">I recently got banned by npm becasue i was looking for package names I could use to publish a program,<br>I knew I wanted something similar to rsync, which is the name of a command responsible for remote synchronization.</div>\n  <div class=\"paragraph\">But somebody else took rsync ages ago, so I started changing the url in the address bar to<br>ssync tsync usync vsync wsync xsync ysync,</div>\n  <div class=\"paragraph\">And all of a sudden the website said, you are doing too much of that,<br>you are temporary banned - fair enough, I guess.</div>\n  <div class=\"paragraph\">I ended up publishing <span>rsend<sup>[16]</sup></span>, because it occurred to me,<br>that I am not really syncing arbitrary locations, but sending from local to remote.</div>\n  <div class=\"paragraph\">I use it, to upload my music to my workout audio player,<br>but the program is very ugly because, I only expect GNU utilities on the target system.</div>\n  <div class=\"paragraph\">While I can program all kinds of things on the local system,<br>the remote system will not have my code, so checking file uploads has to be done via Linux commands.</div>\n</div>\n<div class=\"section avoid-break-inside\" style=\"padding-bottom: 2rem;\">\n  <br>\n</div>\n<div class=\"section avoid-break-inside\" style=\"padding-bottom: 2rem;\">\n  <div class=\"paragraph\">One way to throttle our requests, is to use the sleep command, between downloading each of the stories,<br>we could sleep for 1 second for the first 10 stories, and then, two seconds, three seconds, and finally five seconds between each request, and nobody would get mad a that.</div>\n  <div class=\"paragraph\">Another way to do it, is to pre-fetch three or five stories,<br>ahead of where you are at right now, so you wouldn't have any wait times, if you just read the stories live.</div>\n  <div class=\"paragraph\">If all you want to see for the day is 10 stories,<br>then you will only download 15, AND NOT 500!</div>\n  <div class=\"paragraph\">Beyond that, you could do a little bit of maneuvering,<br>and begin gathering top stories every hour throughout the day at a comfy 10 second interval.</div>\n</div>\n<div class=\"section avoid-break-inside\" style=\"padding-bottom: 2rem;\">\n  <br>\n</div>\n<div class=\"section avoid-break-inside\" style=\"padding-bottom: 2rem;\">\n  <div class=\"paragraph\">There is something you should know,<br>you know how that wc command that we just rand had a -l to make it couldn't lines.</div>\n  <div class=\"paragraph\">When you are inside a full screen program,<br>interacting with things kind of should be maybe about letters as well.</div>\n  <div class=\"paragraph\">If we made an interactive terminal program,<br>that just browsers a screenful at a time.</div>\n  <div class=\"paragraph\">To go to the next page,<br>you would have to pres the \"n\" key.</div>\n  <div class=\"paragraph\">This is yet another Black Screen shortcut,<br>that goes straight into a cheeatsheet.</div>\n  <div class=\"paragraph\">We also need to consider other news sources,<br>whatever your favorite hangout spots are.</div>\n  <div class=\"paragraph\">And, not everyone is going to be as nice as Hacker News and present an API,<br>there may not be a public API.</div>\n  <div class=\"paragraph\">In that case we will need to write a screen scraper,<br>that is one of the worst kinds of programs.</div>\n  <div class=\"paragraph\">As it doesn't use the data beneath the screen,<br>it just sort of does this ugly scrape of whatever is on the screen.</div>\n  <div class=\"paragraph\">This entails controlling a full Web Browser,<br>and sort of making it look like there is a human browsing the page, where it is actually a freaking robot.</div>\n  <div class=\"paragraph\">And it gets worse,<br>lyke super bad.</div>\n  <div class=\"paragraph\">There was a world wide DNS problem couple of days ago,<br>and Amazon keeps forcing me to solve their CAPTCHA when I use Firefox, Chromium works fine for me.</div>\n  <div class=\"paragraph\">There maybe websites out there that can stop you from scraping them,<br>by presenting a CAPTCHA, and the only way to fix that, is to solve it.</div>\n  <div class=\"paragraph\">The glitched Amazon that I am still experiencing,<br>would only ask me to solve the CAPTCHA once per opening a browser, and that was already pretty annoying.</div>\n  <div class=\"paragraph\">CAPTCHA is unlikely to feature on News Websites though,<br>but if you scrape too quickly, of they detect your browser signature you may bet one.</div>\n  <div class=\"paragraph\">Now that all websites use JavaScript, you can't just mirror them with wget,<br>they need an automated browser that will run the JavaScript to make the pages come to life.</div>\n</div>\n<div class=\"section avoid-break-inside\" style=\"padding-bottom: 2rem;\">\n  <br>\n</div>\n<div class=\"section avoid-break-inside\" style=\"padding-bottom: 2rem;\">\n  <div class=\"paragraph\">When we are inventing on the command line,<br>we have to jump ahead of ourselves and see what our program will need when it is finished.</div>\n  <div class=\"paragraph\">And the first thing that comes to mind,<br>is multiple news sources.</div>\n  <div class=\"paragraph\">If we are to create a system to extract interesting stories from multiple places on the internet,<br>then it is definable something that should consist of multiple programs, that run hourly or so.</div>\n  <div class=\"paragraph\">So we would have one program for extracting data from hacker news,<br>another for extracting data from eBay about some obscure phone that is cool but never available.</div>\n  <div class=\"paragraph\">And another program to get emails from different service providers,<br>news from, say, the Raspberry PI blog.</div>\n  <div class=\"paragraph\">And information from Raspberry Pi, like temperature or pressure changes,<br>maybe a new hires photos from the nearby Squirrel cam.</div>\n</div>\n<div class=\"section avoid-break-inside\" style=\"padding-bottom: 2rem;\">\n  <br>\n</div>\n<div class=\"section avoid-break-inside\" style=\"padding-bottom: 2rem;\">\n  <div class=\"paragraph\">We could make this work via a command line RSS reader (with the option of using a proper desktop one),<br>by setting up a local RSS server that keeps track of data from all these places.</div>\n  <div class=\"paragraph\">And I think the data would have to sit in an SQL database,<br>this way all those different data sources would have a universal format that can be easily processed.</div>\n  <div class=\"paragraph\">Once they are put into the database,<br>creating custom RSS feeds is easy, they are actually just plain XML files, same idea as JSON.</div>\n</div>\n<div class=\"section avoid-break-inside\" style=\"padding-bottom: 2rem;\">\n  <br>\n</div>\n<div class=\"section avoid-break-inside\" style=\"padding-bottom: 2rem;\">\n  <div class=\"paragraph\">While SQL and the feed server can be easily can be easily controlled from the command line,<br>the scraping of the various unfriendly websites, will require a custom program.</div>\n  <div class=\"paragraph\">Personally I would base it on <span>Browserless.js<sup>[17]</sup></span>,<br>as it has built-in \"evasions technniques (sic)\" to prevent being blocked.</div>\n  <div class=\"paragraph\">I just noticed that they misspelled evasion techniques,<br>forked their project, and fixed a couple more mistakes I could find.</div>\n  <div class=\"paragraph\"><span>I created a PR, short for pull request, which means that I am requesting that they pull the fixes that I have created<sup>[18]</sup></span>,<br>so because Browserless.js is an Open Source Code Project, anybody can make valuable little contributions.</div>\n  <div class=\"paragraph\">But even with the mighty Browserless.js project,<br>websites we would be scraping will sometimes update their code, and our scraper brains would need to be updated.</div>\n</div>\n<div class=\"section avoid-break-inside\" style=\"padding-bottom: 2rem;\">\n  <br>\n</div>\n<div class=\"section avoid-break-inside\" style=\"padding-bottom: 2rem;\">\n  <div class=\"paragraph\">Overall, our best option here, is to cut 10 entries from top of the Hacker News,<br>and see if we can get away with making 10 quick requests for their top stories, and call our initial version done.</div>\n  <div class=\"paragraph\">Personally I would begin testing <span>Newsbeuter<sup>[19]</sup></span> <span>Snownews<sup>[20]</sup></span> <span>Newsroom<sup>[21]</sup></span> and <span>newsboat<sup>[22]</sup></span> to get the feel for command line news,<br>and consider moving to desktop RSS clients so as long as they support ad blockers.</div>\n  <div class=\"paragraph\">And then quietly, I would begin working on my scrapers,<br>being mindful that they must output RSS feeds, which then I would add to my RSS readers.</div>\n</div>\n<div class=\"section avoid-break-inside\" style=\"padding-bottom: 2rem;\">\n  <br>\n</div>\n<div class=\"section avoid-break-inside\" style=\"padding-bottom: 2rem;\">\n  <div class=\"paragraph\">To be honest, there is not much time that I have for reading internet news,<br>and the more I think about it, the more fetching that little 10 news items program seems, and the more clunky RSS starts to look.</div>\n  <div class=\"paragraph\">Let us chop off the top then entries from Hacker News,<br>and see if we can quickly get an overview of what is going on in the world.</div>\n  <div class=\"paragraph\">The command for getting the top of data is head, and to specify how many lines we want we use -n,<br>so our command now looks like this: curl <span>https://hacker-news.firebaseio.com/v0/topstories.json?print=pretty<sup>[23]</sup></span> | jq '.[]' | head -n 10;</div>\n  <div class=\"paragraph\">Armed with 10 numbers that represent top stories,<br>we now have to take the second step, and use each of those numbers to download the titles from Hacker News.</div>\n  <div class=\"paragraph\">And here we reuse curl, but the url that we give to curl needs to have that number inserted in there,<br>there are two ways to do this, one is xargs, and the other a while loop.</div>\n  <div class=\"paragraph\">The while loop is better, it is more readable, it is faster, but I don't like it because it wraps things,<br>my xargs approach, creates a single straight command line.</div>\n  <div class=\"paragraph\">Unfortunately my line ends up having a command that uses quotes, inside quotes,<br>so there is quote escaping, by means of backslash, and that is something I don't like.</div>\n  <div class=\"paragraph\">But I must say the proper way to do this, is a single pipe,<br>get the top story numbers, map the numbers to data, and map the data to text so that it looks nice on the black screen.</div>\n  <div class=\"paragraph\">Three simple steps,<br>get numbers, convert them to stories, print them to screen.</div>\n  <div class=\"paragraph\">In closing, I will read to you the code of each of the three approaches, in the way that I think about it,<br>I wont read quote, backslash, n, backslash n, but give you a somewhat poetic reading.</div>\n</div>\n<div class=\"section avoid-break-inside\" style=\"padding-bottom: 2rem;\">\n  <br>\n</div>\n<div class=\"section avoid-break-inside\" style=\"padding-bottom: 2rem;\">\n  <div class=\"paragraph\">First is my xargs approach, a single pipe;</div>\n  <div class=\"paragraph\">curl -s <code>https://hacker-news.firebaseio.com/v0/topstories.json</code> | jq '.[]' | head -n 10 | xargs -I % sh -c \"curl -s <code>https://hacker-news.firebaseio.com/v0/item/%.json</code> | jq -r '. | [.title,.url, null] | join(\"\\n\")'\"</div>\n  <div class=\"paragraph\">Now, describe the first loop approach, which is actually a little bit backwards.</div>\n  <div class=\"paragraph\">while read -r ID; do curl -s <code>https://hacker-news.firebaseio.com/v0/item/$ID.json</code> | jq -r '. | [.title, \"\\n\", .url, \"\\n\"] | add'; done &lt; &lt;(curl -s <code>https://hacker-news.firebaseio.com/v0/topstories.json</code> | jq '.[]' | head -n 10 )</div>\n  <div class=\"paragraph\">And here is the way that people probably prefer, but I am unhappy about, because there shoudn't be a loop here.</div>\n  <div class=\"paragraph\">curl -s <code>https://hacker-news.firebaseio.com/v0/topstories.json</code> | jq '.[]' | head -n 10 | while read -r ID ; do curl -s <code>https://hacker-news.firebaseio.com/v0/item/$ID.json</code> | jq -r '. | [.title, \"\\n\", .url, \"\\n\"] | add'; done;</div>\n  <div class=\"paragraph\">And that is what the black screens are all about,<br>I hope I inspired you to look a little bit more into those black screens.</div>\n</div>\n<div class=\"break-after\">&nbsp;</div>\n<div>\n  <div class=\"section\" style=\"padding-bottom: 1rem;\">References</div>\n  <div>[1]: https://linuxcommandlibrary.com/basic/oneliners</div>\n  <div>[2]: https://www.google.com/search?q=Linux+Cheat+Sheet</div>\n  <div>[3]: https://www.guyrutenberg.com/2014/05/02/make-offline-mirror-of-a-site-using-wget/</div>\n  <div>[4]: https://github.com/HackerNews/API</div>\n  <div>[5]: https://hacker-news.firebaseio.com/v0/topstories.json?print=pretty</div>\n  <div>[6]: https://stedolan.github.io/jq/</div>\n  <div>[7]: https://www.youtube.com/watch?v=I6id1Y0YuNk</div>\n  <div>[8]: https://stedolan.github.io/jq/manual/#Basicfilters</div>\n  <div>[9]: https://www.youtube.com/watch?v=FSn_38gDvzM</div>\n  <div>[10]: http://www.catb.org/~esr/jargon/html/R/RTFM.html</div>\n  <div>[11]: https://hacker-news.firebaseio.com/v0/topstories.json?print=pretty</div>\n  <div>[12]: https://www.google.com/search?q=command+line+cheat+sheets</div>\n  <div>[13]: https://www.youtube.com/results?search_query=Linux+command+line+tutorial</div>\n  <div>[14]: https://www.google.com/search?q=linux+pocket+reference</div>\n  <div>[15]: https://hacker-news.firebaseio.com/v0/item/27941616.json?print=pretty</div>\n  <div>[16]: https://github.com/catpea/rsend</div>\n  <div>[17]: https://browserless.js.org/</div>\n  <div>[18]: https://github.com/microlinkhq/browserless/pull/295</div>\n  <div>[19]: https://ostechnix.com/newsbeuter-command-line-rssatom-feed-reader-unix-like-systems/</div>\n  <div>[20]: https://github.com/msharov/snownews</div>\n  <div>[21]: https://www.tecmint.com/newsroom-commandline-linux-news-reader/</div>\n  <div>[22]: https://www.tecmint.com/newsboat-rss-atom-feed-reader-for-linux-terminals/</div>\n  <div>[23]: https://hacker-news.firebaseio.com/v0/topstories.json?print=pretty</div>\n</div>",
  "text": "The shortest possible answer is,\nhttps://linuxcommandlibrary.com/basic/oneliners[1] to get a good overview of what they are typing.\nIt is a different kind of a user interface,\nand the applications within that world can be connected together to make new applications.\nThis used to be the original computer interface,\nbefore graphic applications and the mouse were invented.\nToday it is used because it is faster than the GUI,\nit is less distracting, and it is easier to make those programs.\n\nThink about playing music, how many icons you have to click to get there,\non the command line you just type in cvlc ~/Music/* and keep doing what you were doing.\nThe reason why graphic user interfaces are popular,\nis because they have an easy on ramp, they give you breadcrumbs.\nThe command line requires that you remember where your files are,\nand remember some of the command names, such as cvlc to play music, or wget to download a file.\n\nYou don't have to memorize those commands,\nyou can just download a little cheat-sheet[2].\nAfter a while you will stop forgetting their names,\nit is really not that big of a deal.\nAnd if you need to browse some photos,\nthen you just use a normal GUI program that can display them.\nThough, instead of clicking on an icon somewhere,\nyou'll launch it from the command line by entering its name.\nIt is just another command to add to your cheat sheet,\nand just like there is a limited number of things that you do on your phone.\nYou'll also limit yourself to just knowing 5 to 20 things,\nthat you normally do on the command line.\n\nJust like your phone apps or desktop applications,\nprograms on the command line can do multiple things.\nevery command has a help feature,\nwhere you type in the command name followed by -h and that will list all the different things that a command can do.\nFor example wget has a --mirror option[3],\nhere you don't just download a file, but mirror the entire website.\nSo just imagine wget being an app on your phone, called Web Get,\nand it would have two buttons, \"Download A File\", and \"Download Website\".\n\nNow let me give you a more involved example,\nI would like to show you how to download internet stories.\nEvery website offers some way of getting some of their data,\nHacker News (a website for programmers) has a nice description of their raw data: https://github.com/HackerNews/API[4]\nWe learn that their website data structure is all about numbers,\nand to get at those number we have to download their topstories.json[5]\nJSON is a file format, it is not really purrfect for the command line,\nso we will thin it, reduce it simplify with a program called jq[6],\njq reaches into the JSON file format and grabs plain text for us,\nwithout the extra curly bracets, quotes, or in this case, commas that delimit the entries and square brackets that represent lists of data.\nWe will use curl[7], it is very similar to wget,\nbut it doesn't have that fancy extra stuff for mirroring websites.\nAnd we need a jq tutorial[8] to give us some hints on how to use it, sometimes watching a video tutorial is useful too[9],\nI found the stuff under \"Array/Object Value Iterator: .[]\".\nProgrammers speak like this when they assume only other programmers will read their stuff,\nlittle do they know, we don't really read their funk, we just look at the code.\n\nThat is an important lesson, never read the freaking manual,\nwhat takes you 20 minutes instead of 1 minute, teaches you 20 times more.\nThere is a lot of people out there that say RTFM[10],\nbut that is only because they don't know the answer, because they spent their career brainlessly looking everything up - just ignore them.\n\nSo to get at the numbers we have to say\ncurl https://hacker-news.firebaseio.com/v0/topstories.json?print=pretty[11] | jq '.[]'.\nThose single quotes tell your command line that your code in there belong to jq,\nthe command line is a programming language it self, to keep command related stuff isolated we use single brackets.\n\nThe way you get a feel for this is by looking at command line cheat sheets[12],\nand video tutorials[13], and if you are going somewhere without computers, grab some pocket reference titles[14].\nThe smaller the source of information,\nthe more valuable the information within.\nIf you buy a Linux Bible,\nyou won't be able to tell what is important and what isn't.\nAnd it is really cool to just have a long little row,\nof pocket reference books, it shows that you don't mess around.\n\nAt this point we have a lit of numbers that represent stories on hacker news,\nto grab a story we have to curl item / story number followed by .json 27941616.json[15]\nThere is a bit of a problem here, because piping the output of the list of numbers to,\nwc command with the -l switch, which stands for word count, count number of lines for me.\nReveals that hacker news API returns 500 story numbers,\nand it is bad etiquette to issue 500 requests to hacker news servers.\nMoreover, each hacker news story has, a list of numbers that represent the comments,\npresumably the top comments in the thread.\nEven though they say there is no rate limit,\nwe would probably trigger their security system and get temporary banned.\n\nI recently got banned by npm becasue i was looking for package names I could use to publish a program,\nI knew I wanted something similar to rsync, which is the name of a command responsible for remote synchronization.\nBut somebody else took rsync ages ago, so I started changing the url in the address bar to\nssync tsync usync vsync wsync xsync ysync,\nAnd all of a sudden the website said, you are doing too much of that,\nyou are temporary banned - fair enough, I guess.\nI ended up publishing rsend[16], because it occurred to me,\nthat I am not really syncing arbitrary locations, but sending from local to remote.\nI use it, to upload my music to my workout audio player,\nbut the program is very ugly because, I only expect GNU utilities on the target system.\nWhile I can program all kinds of things on the local system,\nthe remote system will not have my code, so checking file uploads has to be done via Linux commands.\n\nOne way to throttle our requests, is to use the sleep command, between downloading each of the stories,\nwe could sleep for 1 second for the first 10 stories, and then, two seconds, three seconds, and finally five seconds between each request, and nobody would get mad a that.\nAnother way to do it, is to pre-fetch three or five stories,\nahead of where you are at right now, so you wouldn't have any wait times, if you just read the stories live.\nIf all you want to see for the day is 10 stories,\nthen you will only download 15, AND NOT 500!\nBeyond that, you could do a little bit of maneuvering,\nand begin gathering top stories every hour throughout the day at a comfy 10 second interval.\n\nThere is something you should know,\nyou know how that wc command that we just rand had a -l to make it couldn't lines.\nWhen you are inside a full screen program,\ninteracting with things kind of should be maybe about letters as well.\nIf we made an interactive terminal program,\nthat just browsers a screenful at a time.\nTo go to the next page,\nyou would have to pres the \"n\" key.\nThis is yet another Black Screen shortcut,\nthat goes straight into a cheeatsheet.\nWe also need to consider other news sources,\nwhatever your favorite hangout spots are.\nAnd, not everyone is going to be as nice as Hacker News and present an API,\nthere may not be a public API.\nIn that case we will need to write a screen scraper,\nthat is one of the worst kinds of programs.\nAs it doesn't use the data beneath the screen,\nit just sort of does this ugly scrape of whatever is on the screen.\nThis entails controlling a full Web Browser,\nand sort of making it look like there is a human browsing the page, where it is actually a freaking robot.\nAnd it gets worse,\nlyke super bad.\nThere was a world wide DNS problem couple of days ago,\nand Amazon keeps forcing me to solve their CAPTCHA when I use Firefox, Chromium works fine for me.\nThere maybe websites out there that can stop you from scraping them,\nby presenting a CAPTCHA, and the only way to fix that, is to solve it.\nThe glitched Amazon that I am still experiencing,\nwould only ask me to solve the CAPTCHA once per opening a browser, and that was already pretty annoying.\nCAPTCHA is unlikely to feature on News Websites though,\nbut if you scrape too quickly, of they detect your browser signature you may bet one.\nNow that all websites use JavaScript, you can't just mirror them with wget,\nthey need an automated browser that will run the JavaScript to make the pages come to life.\n\nWhen we are inventing on the command line,\nwe have to jump ahead of ourselves and see what our program will need when it is finished.\nAnd the first thing that comes to mind,\nis multiple news sources.\nIf we are to create a system to extract interesting stories from multiple places on the internet,\nthen it is definable something that should consist of multiple programs, that run hourly or so.\nSo we would have one program for extracting data from hacker news,\nanother for extracting data from eBay about some obscure phone that is cool but never available.\nAnd another program to get emails from different service providers,\nnews from, say, the Raspberry PI blog.\nAnd information from Raspberry Pi, like temperature or pressure changes,\nmaybe a new hires photos from the nearby Squirrel cam.\n\nWe could make this work via a command line RSS reader (with the option of using a proper desktop one),\nby setting up a local RSS server that keeps track of data from all these places.\nAnd I think the data would have to sit in an SQL database,\nthis way all those different data sources would have a universal format that can be easily processed.\nOnce they are put into the database,\ncreating custom RSS feeds is easy, they are actually just plain XML files, same idea as JSON.\n\nWhile SQL and the feed server can be easily can be easily controlled from the command line,\nthe scraping of the various unfriendly websites, will require a custom program.\nPersonally I would base it on Browserless.js[17],\nas it has built-in \"evasions technniques (sic)\" to prevent being blocked.\nI just noticed that they misspelled evasion techniques,\nforked their project, and fixed a couple more mistakes I could find.\nI created a PR, short for pull request, which means that I am requesting that they pull the fixes that I have created[18],\nso because Browserless.js is an Open Source Code Project, anybody can make valuable little contributions.\nBut even with the mighty Browserless.js project,\nwebsites we would be scraping will sometimes update their code, and our scraper brains would need to be updated.\n\nOverall, our best option here, is to cut 10 entries from top of the Hacker News,\nand see if we can get away with making 10 quick requests for their top stories, and call our initial version done.\nPersonally I would begin testing Newsbeuter[19] Snownews[20] Newsroom[21] and newsboat[22] to get the feel for command line news,\nand consider moving to desktop RSS clients so as long as they support ad blockers.\nAnd then quietly, I would begin working on my scrapers,\nbeing mindful that they must output RSS feeds, which then I would add to my RSS readers.\n\nTo be honest, there is not much time that I have for reading internet news,\nand the more I think about it, the more fetching that little 10 news items program seems, and the more clunky RSS starts to look.\nLet us chop off the top then entries from Hacker News,\nand see if we can quickly get an overview of what is going on in the world.\nThe command for getting the top of data is head, and to specify how many lines we want we use -n,\nso our command now looks like this: curl https://hacker-news.firebaseio.com/v0/topstories.json?print=pretty[23] | jq '.[]' | head -n 10;\nArmed with 10 numbers that represent top stories,\nwe now have to take the second step, and use each of those numbers to download the titles from Hacker News.\nAnd here we reuse curl, but the url that we give to curl needs to have that number inserted in there,\nthere are two ways to do this, one is xargs, and the other a while loop.\nThe while loop is better, it is more readable, it is faster, but I don't like it because it wraps things,\nmy xargs approach, creates a single straight command line.\nUnfortunately my line ends up having a command that uses quotes, inside quotes,\nso there is quote escaping, by means of backslash, and that is something I don't like.\nBut I must say the proper way to do this, is a single pipe,\nget the top story numbers, map the numbers to data, and map the data to text so that it looks nice on the black screen.\nThree simple steps,\nget numbers, convert them to stories, print them to screen.\nIn closing, I will read to you the code of each of the three approaches, in the way that I think about it,\nI wont read quote, backslash, n, backslash n, but give you a somewhat poetic reading.\n\nFirst is my xargs approach, a single pipe;\ncurl -s https://hacker-news.firebaseio.com/v0/topstories.json | jq '.[]' | head -n 10 | xargs -I % sh -c \"curl -s https://hacker-news.firebaseio.com/v0/item/%.json | jq -r '. | [.title,.url, null] | join(\"\\n\")'\"\nNow, describe the first loop approach, which is actually a little bit backwards.\nwhile read -r ID; do curl -s https://hacker-news.firebaseio.com/v0/item/$ID.json | jq -r '. | [.title, \"\\n\", .url, \"\\n\"] | add'; done < <(curl -s https://hacker-news.firebaseio.com/v0/topstories.json | jq '.[]' | head -n 10 )\nAnd here is the way that people probably prefer, but I am unhappy about, because there shoudn't be a loop here.\ncurl -s https://hacker-news.firebaseio.com/v0/topstories.json | jq '.[]' | head -n 10 | while read -r ID ; do curl -s https://hacker-news.firebaseio.com/v0/item/$ID.json | jq -r '. | [.title, \"\\n\", .url, \"\\n\"] | add'; done;\nAnd that is what the black screens are all about,\nI hope I inspired you to look a little bit more into those black screens.\n \nReferences\n[1]: https://linuxcommandlibrary.com/basic/oneliners\n[2]: https://www.google.com/search?q=Linux+Cheat+Sheet\n[3]: https://www.guyrutenberg.com/2014/05/02/make-offline-mirror-of-a-site-using-wget/\n[4]: https://github.com/HackerNews/API\n[5]: https://hacker-news.firebaseio.com/v0/topstories.json?print=pretty\n[6]: https://stedolan.github.io/jq/\n[7]: https://www.youtube.com/watch?v=I6id1Y0YuNk\n[8]: https://stedolan.github.io/jq/manual/#Basicfilters\n[9]: https://www.youtube.com/watch?v=FSn_38gDvzM\n[10]: http://www.catb.org/~esr/jargon/html/R/RTFM.html\n[11]: https://hacker-news.firebaseio.com/v0/topstories.json?print=pretty\n[12]: https://www.google.com/search?q=command+line+cheat+sheets\n[13]: https://www.youtube.com/results?search_query=Linux+command+line+tutorial\n[14]: https://www.google.com/search?q=linux+pocket+reference\n[15]: https://hacker-news.firebaseio.com/v0/item/27941616.json?print=pretty\n[16]: https://github.com/catpea/rsend\n[17]: https://browserless.js.org/\n[18]: https://github.com/microlinkhq/browserless/pull/295\n[19]: https://ostechnix.com/newsbeuter-command-line-rssatom-feed-reader-unix-like-systems/\n[20]: https://github.com/msharov/snownews\n[21]: https://www.tecmint.com/newsroom-commandline-linux-news-reader/\n[22]: https://www.tecmint.com/newsboat-rss-atom-feed-reader-for-linux-terminals/\n[23]: https://hacker-news.firebaseio.com/v0/topstories.json?print=pretty",
  "images": [],
  "links": [
    {
      "title": "https://linuxcommandlibrary.com/basic/oneliners",
      "url": "https://linuxcommandlibrary.com/basic/oneliners",
      "hostname": "linuxcommandlibrary.com"
    },
    {
      "title": "you can just download a little cheat-sheet",
      "url": "https://www.google.com/search?q=Linux+Cheat+Sheet",
      "hostname": "www.google.com"
    },
    {
      "title": "wget has a --mirror option",
      "url": "https://www.guyrutenberg.com/2014/05/02/make-offline-mirror-of-a-site-using-wget/",
      "hostname": "www.guyrutenberg.com"
    },
    {
      "title": "https://github.com/HackerNews/API",
      "url": "https://github.com/HackerNews/API",
      "hostname": "github.com"
    },
    {
      "title": "topstories.json",
      "url": "https://hacker-news.firebaseio.com/v0/topstories.json?print=pretty",
      "hostname": "hacker-news.firebaseio.com"
    },
    {
      "title": "jq",
      "url": "https://stedolan.github.io/jq/",
      "hostname": "stedolan.github.io"
    },
    {
      "title": "curl",
      "url": "https://www.youtube.com/watch?v=I6id1Y0YuNk",
      "hostname": "www.youtube.com"
    },
    {
      "title": "jq tutorial",
      "url": "https://stedolan.github.io/jq/manual/#Basicfilters",
      "hostname": "stedolan.github.io"
    },
    {
      "title": "sometimes watching a video tutorial is useful too",
      "url": "https://www.youtube.com/watch?v=FSn_38gDvzM",
      "hostname": "www.youtube.com"
    },
    {
      "title": "RTFM",
      "url": "http://www.catb.org/~esr/jargon/html/R/RTFM.html",
      "hostname": "www.catb.org"
    },
    {
      "title": "https://hacker-news.firebaseio.com/v0/topstories.json?print=pretty",
      "url": "https://hacker-news.firebaseio.com/v0/topstories.json?print=pretty",
      "hostname": "hacker-news.firebaseio.com"
    },
    {
      "title": "command line cheat sheets",
      "url": "https://www.google.com/search?q=command+line+cheat+sheets",
      "hostname": "www.google.com"
    },
    {
      "title": "video tutorials",
      "url": "https://www.youtube.com/results?search_query=Linux+command+line+tutorial",
      "hostname": "www.youtube.com"
    },
    {
      "title": "grab some pocket reference titles",
      "url": "https://www.google.com/search?q=linux+pocket+reference",
      "hostname": "www.google.com"
    },
    {
      "title": "27941616.json",
      "url": "https://hacker-news.firebaseio.com/v0/item/27941616.json?print=pretty",
      "hostname": "hacker-news.firebaseio.com"
    },
    {
      "title": "rsend",
      "url": "https://github.com/catpea/rsend",
      "hostname": "github.com"
    },
    {
      "title": "Browserless.js",
      "url": "https://browserless.js.org/",
      "hostname": "browserless.js.org"
    },
    {
      "title": "I created a PR, short for pull request, which means that I am requesting that they pull the fixes that I have created",
      "url": "https://github.com/microlinkhq/browserless/pull/295",
      "hostname": "github.com"
    },
    {
      "title": "Newsbeuter",
      "url": "https://ostechnix.com/newsbeuter-command-line-rssatom-feed-reader-unix-like-systems/",
      "hostname": "ostechnix.com"
    },
    {
      "title": "Snownews",
      "url": "https://github.com/msharov/snownews",
      "hostname": "github.com"
    },
    {
      "title": "Newsroom",
      "url": "https://www.tecmint.com/newsroom-commandline-linux-news-reader/",
      "hostname": "www.tecmint.com"
    },
    {
      "title": "newsboat",
      "url": "https://www.tecmint.com/newsboat-rss-atom-feed-reader-for-linux-terminals/",
      "hostname": "www.tecmint.com"
    }
  ]
}